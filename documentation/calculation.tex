\documentclass[letter,12pt]{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{autonum}
\usepackage{enumitem}
\usepackage{titlesec}
\usepackage{parskip}
\usepackage{caption}
\usepackage{tikz}
\usepackage{siunitx}
\usepackage{titletoc}
\usepackage{authblk}
\usepackage{lineno}
\usepackage{etoolbox}
\cslet{blx@noerroretextools}\empty
\usepackage[style=nature]{biblatex}
%% \linenumbers


\newcommand*\patchAmsMathEnvironmentForLineno[1]{%
  \expandafter\let\csname old#1\expandafter\endcsname\csname #1\endcsname
  \expandafter\let\csname oldend#1\expandafter\endcsname\csname end#1\endcsname
  \renewenvironment{#1}%
                   {\linenomath\csname old#1\endcsname}%
                   {\csname oldend#1\endcsname\endlinenomath}}% 
\newcommand*\patchBothAmsMathEnvironmentsForLineno[1]{%
  \patchAmsMathEnvironmentForLineno{#1}%
}%
\AtBeginDocument{%
  \patchBothAmsMathEnvironmentsForLineno{equation}%
  \patchBothAmsMathEnvironmentsForLineno{align}%
  \patchBothAmsMathEnvironmentsForLineno{flalign}%
  \patchBothAmsMathEnvironmentsForLineno{alignat}%
  \patchBothAmsMathEnvironmentsForLineno{gather}%
  \patchBothAmsMathEnvironmentsForLineno{multline}%
}


%% \title{Solutions to the representation assignment problem balance a
%%   trade-off between local and catastrophic errors}
%% \author[1,2,*]{\small W. Jeffrey Johnston}
%% \author[1,2]{\small David J. Freedman}

%% \affil[1]{\footnotesize Graduate Program in Computational Neuroscience}
%% \affil[2]{\footnotesize Department of Neurobiology}
%% \affil[ ]{\small The University of Chicago, Chicago, IL, USA}
%% \affil[*]{\footnotesize Corresponding author:
%%   \href{mailto:wjeffreyjohnston@gmail.com}{wjeffreyjohnston@gmail.com}}

\addbibresource{/Users/wjj/Dropbox/research/uc/freedman/papers/papers.bib}

\newcommand*{\nref}[2]{% 
  \hyperref[{#2}]{\emph{\nameref*{#2}} in \emph{\nameref*{#1}}}}
\newcommand*{\kref}[2]{%
  \cref{#1}#2}
\crefformat{equation}{Eq.~#2#1#3}
\crefformat{figure}{Figure~#2#1#3}

\newtheorem{statement}{Statement}
\renewcommand*{\proofname}{Derivation}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\sign}{sgn}

\newcommand{\dll}{d_{LL}}
\newcommand{\dlg}{d_{LG}}
\newcommand{\dla}{d_{LA}}
\newcommand{\dn}{d_{N}}
\newcommand{\dis}{\mathcal{N}(0, 1/D)}
\newcommand{\fno}{f_{2N}^{\text{opt}}}
\newcommand{\E}{\mathbb{E}}

% \doublespacing

\begin{document}
\captionsetup[figure]{labelfont=bf,textfont=normalfont,
  singlelinecheck=off,justification=raggedright,font=footnotesize}
\pagenumbering{gobble}
\setlength{\extrarowheight}{5pt}

%% \maketitle

\section{Analysis of discrete linear-nonlinear codes}

\subsection{Code definition}
We consider the following family of code,
\begin{align}
  r = f_{L}(x) + f_{IK}(x) + \epsilon
\end{align}
where the $N$-dimensional neural response $r$ to stimulus $x \in [1, ..., n]^{K}$
is modeled in terms of a linear part, $f_{L}(x)$, a nonlinear part, $f_{NL}(x)$,
and additive Gaussian noise, $\epsilon \sim \mathcal{N}(0, \sigma^{2} I_{N})$.
The linear part has the form, 
\begin{align}
  f_{L}(x) = M x_{\textrm{z}}
\end{align}
where $M$ is an $N \times K$ matrix and $x_{\textrm{z}}$ is z-scored version of
$x$. The magnitude of vectors in $M$ are chosen so that
$\E_{x} f_{L}(x)^{2} = P_{L}$.
The nonlinear part has the form,
\begin{align}
  f_{NL}(x)_{i} = \sqrt{P_{NL}} n_{i} [x_{1} = t_{i}(1)] ... [x_{K} = t_{i}(K)]
\end{align}
for $i \in [1, ..., n^{K}]$ where $[...]$ is the indicator function,
$t_{i}$ returns a values in $[1, ..., K]$, and $n_{i}$ is a unit vector chosen
so that $n_{i} \cdot n_{j} = 0 \, \forall \, i \ne j$. That is, there is a dimension in
$f_{NL}$ that is one for a single, unique stimulus and zero for all other stimuli.

A set of stimuli is defined by two parameters: $K$, the number of distinct
features, or latent variables; $n$, the number of a discrete values that each of
the $K$ features takes on. There are $n^{K}$ unique stimuli. A code is described
by four parameters: $P_{L}$ and $P_{NL}$ are the power of the linear and
nonlinear code parts, respectively -- the total power is $P = P_{L} + P_{NL}$;
the number of neurons $N$; and the noise variance $\sigma^{2}$. The
$\textrm{SNR} = \sqrt{P/\sigma^{2}}$. 

\textbf{Current results:}
\begin{enumerate}
\item Analytic approximation of the error rate.
\item Analytic worst-case lower bound on XOR performance.
\item Analytic approximation of the CCGP.
\end{enumerate}

\textbf{Current applications:}
\begin{enumerate}
\item Can show the SNR required for both high flexibility and CCGP for a
  given stimulus set.
\item Can simulate responses to multiple simultaneous stimuli and get error
  rate. 
\end{enumerate}

\textbf{Possible extensions/applications:}
\begin{enumerate}
\item Can show how many examples are necessary to learn a generalizable category
  boundary (a la Ben's paper).
\item Can make claims about modularity for some kinds of tasks. 
\item Can use to model transfer in perceptual learning.
\item Can characterize tradeoff between CCGP and multi-stimulus capacity
  (high multi-stimulus capacity may imply low CCGP). 
\item Additional code parts (e.g., $f_{I1}$).
\item Continuous stimuli.
\end{enumerate}

\subsection{Preliminaries}
To estimate the error rate and other quantities in these codes, we need to
understand how the code parameters scale the distances in representation space
between nearby stimuli. This distance will be important for deriving error and
generalization rates. We do this separately for the different code parts and
show that the distances are close to additive later.

\subsubsection{The linear code part}
In the linear code, all stimuli lie on a $K$-dimensional rectangular lattice.
We derive the distance between points on that lattice as a function of our
stimulus and code parameters, as well as the number of neighbors a
representation has at minimum and next-from-minimum distance.

\textbf{Linear distance derivation:}
First, we find how the distance between
adjacent stimuli in the linear code depends on the number of features $K$ and
the number of values that each feature takes on $n$ along with the linear power
of the code ($P_{L}$),
\begin{align}
d_{L} &= \sqrt{\frac{12 P_{L}}{K (n^2 - 1)}}
\end{align}

We approach this by computing the variance (i.e., the linear power $P_{L}$)
of a uniformly sampled $K$-dimensional lattice with $n$ points spaced at distance
$d_{L}$ along each dimension. Then, we
invert the expression for the variance to find an expression for the distance
between the points. First, we write the variance $P_{L}$ as
\begin{align}
  n^{K} P_{L} &=  \sum_{i = 0}^{n - 1} \left[
    \left(i - \frac{n - 1}{2}\right)^{2}d_{L}^{2} + \sum_{j = 0}^{n - 1}
    \left[\left(j - \frac{n - 1}{2}\right)^{2}d_{L}^{2} + ... \right]\right] \\
  &= \sum_{i}^{n - 1} \sum_{j}^{n-1} ... \sum_{k}^{n - 1}
  \left(i - \frac{n - 1}{2}\right)^{2}d_{L}^{2} + 
  \left(j - \frac{n - 1}{2}\right)^{2}d_{L}^{2} + ... +
  \left(k - \frac{n - 1}{2}\right)^{2}d_{L}^{2} \\
  &= K n^{K - 1} \sum_{i}^{n - 1}
  \left(i - \frac{n - 1}{2}\right)^{2}d_{L}^{2} \\
  &= K n^{K - 1} d_{L}^{2}\sum_{i}^{n - 1} i^{2} - (n - 1) \sum_{i}^{n-1} i
  + n \frac{n - 1}{2}^{2}
\end{align}
and we can rewrite this with known expressions for the sum of integers
and sum of squared integers up to a particular value,
\begin{align}
  n^{K} P_{L} &= K n^{K - 1} d_{L}^{2}
  \left[\frac{(n - 1)n(2n - 1)}{6} - \frac{n(n - 1)^{2}}{2}
    + \frac{n(n - 1)^{2}}{4}\right] \\
  &= K n^{K} d_{L}^{2} \left[\frac{(n - 1)(2n - 1)}{6} - \frac{(n - 1)^{2}}{4}
    \right] \\
  &= K n^{K} d_{L}^{2} \left[\frac{2n^{2} - 3n + 1}{6}
    - \frac{n^{2} - 2n + 1}{4}\right] \\
  &= K n^{K} d_{L}^{2} \left[\frac{4n^{2} - 6n + 2}{12}
    - \frac{3n^{2} - 6n + 3}{12}\right] \\
  n^{K} P_{L} &= K n^{K} d_{L}^{2} \frac{n^{2} - 1}{12} \\
  P_{L} &= K d_{L}^{2} \frac{n^{2} - 1}{12} 
\end{align}
Now, we rewrite in terms of $d_{L}$,
\begin{align}
  d_{L} &= \sqrt{\frac{12 P_{L}}{K \left(n^{2} - 1\right)}}
\end{align}
which is the expression given above.

Following from the lattice structure, stimuli at a diagonal point on the lattice
have distance $\sqrt{2} d_{L}$.

\textbf{Linear neighbors derivation:}
Second, we find the average number of neighbors that a particular stimulus has at
both this nearest distance $N_{LA}$ and nearest diagonal distance $N_{LD}$. This
is a counting problem. We observe that, in the lattice, there are two edge values
for each feature and $n - 2$ non-edge values. Thus,
\begin{align}
  N_{LA} &= \frac{1}{n^{K}}
  \sum_{c = 0}^{K} (2K - c)\binom{K}{c} (n - 2)^{K - c} 2^{c}
\end{align}
and
\begin{align}
  N_{LD} &= \frac{1}{n^{K}} \sum_{c = 0}^{K} \left(4\binom{K - c}{2} + 2(K - c)c
  + \binom{C}{2}\right)
  \binom{K}{c} (n - 2)^{K - c} 2^{c}
\end{align}

\textbf{Nonlinear distance derivation:}
The nonlinear distance has been treated in detail elsewhere (for any $f_{Ic}$ with
$c \in [1, ..., K]$). The nonlinear distance is
\begin{align}
  d_{N} &= \sqrt{2 P_{N}}
\end{align}

\textbf{Nonlinear neighbors derivation:}
Because each nonlinear representation is along a vector that is orthogonal to all
other nonlinear representations, from a particular stimulus all other
representations are at minimum distance. So,
\begin{align}
  N_NL &= n^{K} - 1
\end{align}

\textbf{Total code distance:}
Naively, the total code distance would be
\begin{align}
  d_{C} &= \sqrt{d_{L}^{2} + d_{NL}^{2}}
\end{align}
However, because the linear and nonlinear parts are chosen to project into random
subspaces, there is the chance of some alignment. Thus, the corrected total code
distance is a random variable with the following form,
\begin{align}
  d_{C} &= \sqrt{d_{L}^{2} + d_{NL}^{2} + 2 d_{NL} d_{L} \eta}
\end{align}
where $\eta \sim \mathcal{N}(0, 1/N)$ due to the fact that the dot product of
two unit vectors are normally distributed with variance inverse to their length
(i.e., the distribution of $\eta$). The distance is similary defined for the
next nearest stimuli,
\begin{align}
  d_{C + 1} &= \sqrt{2d_{L}^{2} + d_{NL}^{2} + \sqrt{8} d_{NL} d_{L} \eta}
\end{align}

\textbf{Total code neighbors:}
To combine the code neighbors, it is enough to simply take the minimum between
the linear and nonlinear parts, which will always be equal to $N_{LA}$ (or $N_{LD}$
for next-nearest). So,
\begin{align}
  N_{C} &= N_{LA} \\ 
  N_{C + 1} &= N_{LD}
\end{align}

\textbf{Nonlinear distance as a function of linear distance:}
It is of interest to compute how, for fixed power, linear and nonlinear
distance depend on each other. So,
\begin{align}
  P &= P_{N} + P_{L} \\
  &= \frac{d_{N}^{2}}{2} + K d_{L}^{2} \frac{n^{2} - 1}{12} \\
  d_{N}^{2} &= 2 P - 2 K d_{L}^{2} \frac{n^{2} - 1}{12}\\
  d_{N} &= \sqrt{2P - 2 K d_{L}^{2} \frac{n^{2} - 1}{12}}
\end{align}

\subsection{Approximating code error rate}
First, we ask how often this family of codes will make errors. Here, we
define an error as the most likely stimulus under a maximum likelihood decoder
$\hat{x}$ not being the original stimulus $x$. Following the logic that the
nearest stimuli are most likely to be errored toward, we develope the following
expression for the error rate,
\begin{align}
  P(\textrm{error}) &= N_{C} Q\left(-\frac{d_{C}}{2\sigma}\right)
  + N_{C + 1} Q\left(-\frac{d_{C + 1}}{2\sigma}\right)
\end{align}
Thus, we can see that the error rate depends most strongly on the distances.
From our distance definitions, we know that increasing nonlinear power is the
most efficient way to increase distance. As a consequence, to drive the error
rate down, it is best to put all code power toward the nonlinear part. 

For multiple stimuli, we hypothesize that we can write the code error rate as,
\begin{align}
  P(\textrm{error}) &= S N_{C} Q\left(-\frac{d_{C}}{2\sigma}\right)
  + S N_{C + 1} Q\left(-\frac{d_{C + 1}}{2\sigma}\right)
  + N_{S} Q\left(-\frac{d_{S}}{2\sigma}\right)
\end{align}
where $N_{S}$ is the number of equiprobable stimulus sets under the linear part
of the code and $d_{S} = 2\sqrt{P_{N}}$ (maybe).

\subsubsection*{The number of swaps}
\begin{align}
  N_{S} &= \frac{1}{2} \binom{S}{2} \sum_{i = 0}^{K - 1}
  \binom{K}{i} \frac{1}{n}^{i} \left(1 - \frac{1}{n}\right)^{K - i}
  \sum_{j = 1}^{K - i} \binom{K - i}{i}
\end{align}

\subsection{Approximating code flexibility}
We consider a code for a discrete set of stimuli to be flexible if arbitrary
binary partitions of that set can be read out with a linear decoder. When a code
has a mixture of linear and nonlinear stimulus representations, some partitions
are orthogonal to the linear structure in the representation and can be
implemented only if the nonlinear components of the representation are strong
enough -- one such partition is the parity or XOR partition. Thus, to approximate
code flexibility, we will focus on this case. It allows us to ignore any
contribution to the representation from the linear code and focus only on the
nonlinear code. Further, we will take a lower bound on this case. 

In the nonlinear code for $n^{K} = N_{s}$ stimuli, all of the stimuli are
$\sqrt{P_{N}}$ from the origin in representation space and $\sqrt{2P_{N}}$ from
each other. In this case, the vector corresponding to the optimal hyperplane for
a linear decoder that
implements an arbitrary partition of such stimuli has a constant magnitude
$c$ in the direction of all stimuli -- and the magnitude is positive for
stimuli in one category and negative for stimuli in the other category. Using
this understanding, we can calculate the performance of the linear decoder where
$r$ is the decoding vector, $x$ is particular stimulus representation in the
positive category, and $\sigma^{2}$ is the variance of normally distributed output
noise for the neurons in the code:
\begin{align}
  E_{f} &\geq P(r \dot x > 0) \\
  &= P(\mathcal{N}(\sqrt{P_{N}}, N_{s} \sigma^{2}) > 0) \\
  &= Q\left(-\frac{\sqrt{P_{N}}}{\sqrt{N_{s}\sigma^{2}}}\right) \\
  &= Q\left(-\frac{\sqrt{P_{N}}}{n^{K/2} \sigma}\right) \\
\end{align}
where $Q$ is the cumulative distribution function of the standard normal
distribution.

\subsection{Approximating code generalization}
We consider a code to have good generalization performance when a linear decoder
aligned with some combination of code features that is learned on one part of the
stimulus space provides good performance on another part of the stimulus space.
In a simple case with two stimulus features that each take on two values, this
means that a linear decoder that discriminates the value of the second feature
(0 or 1) learned for a fixed value of the first feature (say, 0) will generalize
with minimal loss of performance to other values of the first feature (in this
case, 1). This notion of generalization performance is referred to as
cross-condition generalization performance (CCGP).

We set out to approximate CCGP with a pair of stimuli used for training and
a third stimulus that must be generalized to. Here, we consider a linear code
that is distorted by a nonlinear code.
Thus, we can consider distances in the purely linear code and distances in the
purely nonlinear code separately.

We use $\dll$ to denote the distance in the linear code between the two
stimulus representations used to learn the classification. This distance is
along the unit vector $f_{1}$. Further, we use $\dlg$ to denote the distance
along that unit vector $f_{1}$ of the third stimulus $s_{3}$ which is generalized
to. We use $\dla$ to
denote the distance between the pair of stimuli that is used for training and
the third stimulus along the axis that they are
to be generalized over, which we denote as the unit vector $f_{2}$. Each of the
stimuli also undergoes a distortion of magnitude $\sqrt{P_{N}}$ due to the
nonlinear code, we denote the direction of these distortions as the unit
vectors $n_{i}$. They are chosen such that
$n_{i} \cdot n_{j} = 0$ for $i \ne j$ -- however, $n_{i} \cdot f_{j}$ is not
constrained to be zero. From above, we know that
$n_{i} \cdot f_{j} \sim \dis$ where $D$ is the
full dimensionality of the space (i.e., the number of neurons in the code).
Additionally, for convenience, we also use $n_{ij}$ for any number of indices
to refer to the following
\begin{align}
  n_{ij} &= \frac{n_{i} + n_{j}}{\sqrt{2}}
\end{align}
and similarly for more indices, so that the end vector is a unit vector.

\begin{figure}[ht!]
  \begin{center}
    \includegraphics{../figs/hand-schematic}
  \end{center}
  %% figures: legends <250 words, should be understandable in isolation from
  %% main text
  \caption[Illustration of the idea behind the CCGP approximation.]
          {Illustration of the idea behind the CCGP approximation.
            The purple points are the representations used for training, the
            blue are those to be generalized to. The representations used
            for training define a hyperplane (red) that depends on both the
            linear ($f_{1}$) and nonlinear ($n_{1}$ and $n_{2}$) parts of
            the code. To approximate CCGP, we can simply take the dot product
            between the vector defined by this hyperplane and the positions of
            the other representations (blue). Thus, we find distance
            $d_{3} = f_{1N} \cdot s_{3} - \frac{c}{2}$. 
  }
  \label{schem}
\end{figure}

First, we find the center points between our two pairs of stimuli in the full
code with reference to the ``bottom left'' stimulus, $s_{2}$. In particular,
$s_{1} = f_{1} \dll + \dn n_{12}$, $s_{2} = 0$, and
$s_{3} = \dlg f_{1} + \dla f_{2} + \dn n_{23}$. Thus, 
\begin{align}
  \hat{s}_{12} &= \frac{1}{2}\left(\dll f_{1} + \dn n_{12}\right)
\end{align}

Next, we find the vector pointing between the two representations used for
learning (that is, $s_{1}$ and $s_{2}$), which is given by
\begin{align}
  f_{1N} &= \frac{1}{c}\left(s_{1} - s_{2}\right) \\
  f_{1N} &= \frac{1}{c}\left(\dll f_{1} + d_{n}n_{12}\right)
\end{align}
where $c$ is a random variable that normalizes $f_{1N}$ to be a unit
vector, and corresponds to the distance between $s_{1}$ and $s_{2}$,
\begin{align}
  c &= \sqrt{\dll^{2} + \dn^{2} + 2 \dll \dn f_{1} \cdot n_{12}} \\
  &= \sqrt{\dll^{2} + \dn^{2} + 2 \dll \dn \dis} 
\end{align}

Now, using $f_{1N}$, $c$, along with our understanding of $s_{3}$
as a linear combination of linear and nonlinear codes, we can directly
approximate CCGP. To do this, we need to find the position
of $s_{3}$ along the decoding vector defined by $f_{1N}$, and then
we evaluate whether that magnitude is greater or smaller than the threshold
$c/2$. So, to find this distance relative to the threshold, we need $d_{3}$ such
that
\begin{align}
  d_{3} &= f_{1N} \cdot s_{3} - \frac{c}{2} \\
  &= \frac{1}{c}\left(\dll f_{1} + \dn n_{12}\right)
  \left(\dlg f_{1} + \dla f_{2} + \dn n_{23}\right) - \frac{c}{2} 
\end{align}
First, we focus on the first term and drop $c$ for now,
\begin{align}
  t_{1} &= \left(\dll f_{1} + \dn n_{12}\right)
  \left(\dlg f_{1} + \dla f_{2} + \dn n_{23}\right) \\
  &= \dll\dlg + \dll \dn f_{1} n_{23} + \dlg \dn f_{1} n_{12}
  + \dla \dn f_{2} n_{12} + \dn^{2} n_{23} n_{12} \\
  &= \dll\dlg + \frac{1}{2}\dn^{2}
  + \frac{\dn}{\sqrt{2}}\left(\dlg f_{1}n_{1} + \dlg f_{1}n_{2}
  + \dll f_{1}n_{2} + \dll f_{1}n_{3}\right) + \dla \dn n_{12}f_{2} 
\end{align}
Next, we bring back the full expression and multiply everything by $c$,
\begin{align}
  c d_{3} &= \dll\dlg + \frac{1}{2}\dn^{2}
  + \frac{\dn}{\sqrt{2}}\left(\dlg f_{1}n_{1} + \dlg f_{1}n_{2}
  + \dll f_{1}n_{2} + \dll f_{1}n_{3}\right) + \dla \dn n_{12}f_{2}
  - \frac{c^{2}}{2} \\
  &= \dll\dlg + \frac{1}{2}\dn^{2}
  + \frac{\dn}{\sqrt{2}}\left(\dlg f_{1}n_{1} + \dlg f_{1}n_{2}
  + \dll f_{1}n_{2} + \dll f_{1}n_{3}\right) + \dla \dn n_{12}f_{2}
  \\ &- \frac{1}{2}\dll^{2} - \frac{1}{2} \dn^{2}
  - \frac{\dll\dn}{\sqrt{2}}\left(f_{1}n_{1} + f_{1}n_{2}\right) \\
  &= \dlg\dll - \frac{1}{2}\dll^{2} + (\dlg - \dll)\dn f_{1}n_{12} +
  \dll\dn f_{1}n_{23} + \dla\dn f_{2}n_{12} \\
  d_{3} &= \frac{\dlg\dll - \frac{1}{2}\dll^{2} + (\dlg - \dll)\dn f_{1}n_{12} +
  \dll\dn f_{1}n_{23} + \dla\dn f_{2}n_{12}}
  {\sqrt{\dll^{2} + \dn^{2} + 2 \dll \dn f_{1}n_{12}}} \\
  d_{3} &\sim \frac{\dlg\dll - \frac{1}{2}\dll^{2} + (\dlg - \dll)\dn \dis +
  \dll\dn \dis + \dla\dn \dis}
  {\sqrt{\dll^{2} + \dn^{2} + 2 \dll \dn \dis}}
\end{align}
and this $d_{3}$ relates to the CCGP error rate in the following way,
\begin{align}
  P(\textrm{CCGP error}_{1}) &= \mathbb{E}_{d_{3}} \; Q\left(-d_{3}/\sigma\right)
\end{align}
Thus, making $d_{3}$ as large as possible will minimize CCGP errors. 
%% \begin{align}
%%   P(\textrm{CCGP error}) &= \mathbb{E}_{d} \; Q\left(d/\sigma\right) \\
%%   &= \mathbb{E} \; Q\left(\frac{1}{\sigma}
%%   \frac{\dlg\dll - \frac{1}{2}\dll^{2} + (\dlg - \dll)\dn \dis +
%%   \dll\dn \dis + \dla\dn \dis}
%%   {\sqrt{\dll^{2} + \dn^{2} + 2 \dll \dn \dis}}\right)
%% \end{align}

For $D$ much larger than any of $\dlg^{2}$, $\dla^{2}$, $\dll^{2}$, we can
further simplify by ignoring the random variables
(which is also the case where the nonlinear and linear codes are perfectly
orthogonal to each other),
\begin{align}
  P(\textrm{CCGP error}) &\approx Q\left(-\frac{1}{\sigma}
  \frac{\dlg\dll - \frac{1}{2}\dll^{2}}
  {\sqrt{\dll^{2} + \dn^{2}}}\right)
\end{align}
Next, we can rewrite this in terms the number of features $K$, the number of
values $n$ taken on by each feature, the steps on the lattice separating
the trained pair from each other $i$ (i.e., $\dll = i d_{L}$) and the
generalized stimulus from the
reference stimulus $j$ (i.e., $\dlg = j d_{L}$), as well as the linear $P_{L}$
and nonlinear $P_{N}$ power. So,
\begin{align}
  P(\textrm{CCGP error}) &\approx Q\left(-\frac{12 P_{L}}{K (n^{2} - 1)\sigma}
  \frac{ij - \frac{1}{2}i^{2}}
  {\sqrt{i^{2}\frac{12 P_{L}}{K(n^{2} - 1)} + 2 P_{N}}}\right)  
\end{align}

%% Now, we find the optimal $f_{2}$ in the disorted code, which is also the category
%% boundary that would be learned by a decoder trained with both stimulus
%% representation pairs. This is given by,
%% \begin{align}
%%   f_{2N}^{\text{opt}} &= \hat{s}_{34} - \hat{s}_{12} \\
%%   &= \frac{1}{a}\left(\dla f_{2} + \frac{\sqrt{2}}{2} \dn n_{1234}\right)
%% \end{align}
%% where $a$ is a random variable given by
%% \begin{align}
%%   a &= \sqrt{\dla^{2} + \frac{1}{2}\dn^{2} \sqrt{2}\dn \dll \dis}
%% \end{align}
%% which is the distance between the centers of the two sets of points.

%% The decoding hyperplane used by a decoder trained on only the first set of
%% stimulus representations is given by the unit vector orthogonal to it, which is
%% simply $f_{1N}$. If $f_{1N} \cdot f_{2N} = 0$, then the decoding hyperplane
%% includes $f_{2N}^{\text{opt}}$ and generalization performance will be as good as
%% possible. Thus, we can approximate CCGP by studying the dot product of these
%% two vectors. The dot product can be written as,
%% \begin{align}
%%   b &= f_{2N}^{\text{opt}} \cdot f_{1N} \\
%%   &= \frac{1}{ac} \sqrt{\frac{3}{2}} \dn \dla \dis
%% \end{align}
%% Geometrically, $b \frac{c}{2}$ is the distance along $\fno$ that $s_{1}$ and
%% $s_{2}$ are from the center point $\hat{s}_{12}$. Here, we have two sides of a
%% right triangle, the hypotenuse has length $c/2$ and the other with length
%% $b \frac{c}{2}$. We use these to find the angle between the learned hyperplane
%% and the optimal hyperplane,
%% \begin{align}
%%   \theta &= \frac{\pi}{2} - \arccos\left(\frac{2b}{c}\right) \\
%%   &= \arcsin\left(\frac{2b}{c}\right)
%% \end{align}
%% In most cases, the larger $\theta$, the worse generalization performance will
%% be.

%% \begin{align}
%%   a_{3,4} &= a \pm \frac{\sqrt{2}}{2a}\dn \left(\frac{1}{2}\dla \dis
%%   + \dll \dis\right)
%% \end{align}
%% where the second term captures the distortion relative to the mean distance
%% between the two stimuli. Then, we can find the distance of $s_{i}$ for $i \in
%% [3, 4]$ from the learned decoding hyperplane by finding
%% \begin{align}
%%   d_{3} &= f_{1N} s_{3} - \frac{c}{2}
%% \end{align}

\end{document}
